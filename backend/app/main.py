# main.py
import os, io, uuid, zipfile
import logging, traceback
from typing import List
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from pathlib import Path
import json, time, re, unicodedata

# âœ… .env ìë™ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# âœ… ë¹„ë™ê¸° ì¹œí™”: ë™ê¸° í•¨ìˆ˜ ì›Œì»¤ ìŠ¤ë ˆë“œë¡œ
import anyio

from app.services.ocr import ocr_funnel_extract, batch_ocr_zip
from app.services.llm import summarize_and_categorize
from app.utils.version import get_version
from app.utils.telemetry import Telemetry, PerfRecorder


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("app")

APP_NAME = "ocr-llm-suite"
SESS_ROOT = "tmp/sessions"

app = FastAPI(title=APP_NAME)

# === ì›ë³¸ ì €ì¥ì†Œ ìœ í‹¸ ===
STORE_DIR = Path("tmp/ocr_store")
STORE_DIR.mkdir(parents=True, exist_ok=True)

def _store_doc(doc_id: str, payload: dict):
    (STORE_DIR / f"{doc_id}.json").write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")

def _load_doc(doc_id: str) -> dict:
    p = STORE_DIR / f"{doc_id}.json"
    if not p.exists():
        raise FileNotFoundError(doc_id)
    return json.loads(p.read_text(encoding="utf-8"))

# ================================================================
# ğŸ‘‡ Ollama í™˜ê²½ë³€ìˆ˜ í™•ì¸ ë¡œê·¸ (ì„œë²„ ì‹¤í–‰ ì‹œ ì½˜ì†”ì— í‘œì‹œ)
# ================================================================
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
logger.info(f"ğŸ§  Using Ollama model: {OLLAMA_MODEL}")
logger.info(f"ğŸŒ Ollama host: {OLLAMA_HOST}")
# ================================================================

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === ìš”ì²­ ë°”ë”” ëª¨ë¸ ===
class CompareRequest(BaseModel):
    left_text: str
    right_text: str

class CompareByIdRequest(BaseModel):
    left_id: str
    right_id: str
    mode: str = "text"  # í˜„ì¬ëŠ” ì›ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë¹„êµ. ì¶”í›„ visual(SSIM) í™•ì¥ ê°€ëŠ¥.

# === í—¬ìŠ¤/ë²„ì „ ===
@app.get("/healthz")
def healthz():
    return {"status": "ok"}

@app.get("/version")
def version():
    return {"version": get_version(), "name": APP_NAME}

# === ì„œë²„ ê¸°ë™ ì‹œ í”„ë¦¬ì›Œë° ===
@app.on_event("startup")
async def _warm_llm():
    try:
        logger.info("ğŸ”¥ Warming up LLM model...")
        # ì›Œë°ì—…ì€ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ ìŠ¤ë ˆë“œë¡œ
        await anyio.to_thread.run_sync(summarize_and_categorize, "warmup")
        logger.info("ğŸ”¥ LLM warmup complete.")
    except Exception as e:
        logger.warning(f"ğŸ”¥ LLM warmup failed: {e}")

# === ì—…ë¡œë“œ: OCR -> LLM -> ì €ì¥(doc_id) ===
@app.post("/api/v1/ocr/upload")
async def upload(files: List[UploadFile] = File(...)):
    perf = PerfRecorder(enabled=os.getenv("PERF_ENABLED", "false").lower() == "true")
    telemetry = Telemetry()
    results = []

    for f in files:
        content = await f.read()
        key = f.filename or f"file-{uuid.uuid4()}.bin"

        logger.info(f"ğŸ“¥ RECEIVED FILE: {key}, size={len(content)} bytes")

        try:
            with perf.step(f"ocr:{key}"):
                logger.info(f"ğŸ” START OCR: {key}")
                # â¬‡ï¸ OCRì€ ë™ê¸° í•¨ìˆ˜ (ë¬¸ì œ ì—†ìŒ)
                text, pages, meta, per_page_texts = ocr_funnel_extract(content, filename=key, mode="quality")
                logger.info(f"âœ… DONE OCR: {key} ({pages} pages)")
        except Exception as e:
            logger.exception(f"âŒ OCR failed for {key}: {e}")
            raise HTTPException(status_code=500, detail=f"OCR failed for {key}: {e}")

        try:
            with perf.step(f"llm:{key}"):
                logger.info(f"ğŸ§  START LLM: {key}")
                # â¬‡ï¸ ë™ê¸° summarizeë¥¼ ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€
                summary = await anyio.to_thread.run_sync(summarize_and_categorize, text)
                logger.info(f"âœ… DONE LLM: {key}")
        except Exception as e:
            logger.exception(f"âŒ LLM failed for {key}: {e}")
            raise HTTPException(status_code=500, detail=f"LLM failed for {key}: {e}")

        # âœ… OCR/LLM ëë‚œ ì§í›„ ì›ë³¸ë¬¸ ì €ì¥ + doc_id ìƒì„± (meta/per_page í¬í•¨)
        doc_id = str(uuid.uuid4())
        _store_doc(doc_id, {
            "doc_id": doc_id,
            "filename": key,
            "pages": pages,
            "full_text": text,
            "per_page": per_page_texts,  # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸/í…ìŠ¤íŠ¸ ë ˆì´ì–´ ì—¬ë¶€
            "meta": meta,                # coverage, mode ë“±
            "summary": summary,
            "ts": int(time.time()),
        })

        # âœ… ì‘ë‹µì—ë„ doc_id í¬í•¨
        results.append({
            "doc_id": doc_id,
            "filename": key,
            "pages": pages,
            "rawTextPreview": (text[:1200] + "...") if len(text) > 1200 else text,
            "summary": summary,
        })

    telemetry.merge(perf.to_telemetry())
    logger.info("ğŸ“¦ Upload batch complete. Returning response.")
    # OCR ëë‚œ ì§í›„
    logger.info(f"ğŸ§¾ OCR text length: {len(text)} chars; preview={repr((text or '')[:120])}")

    return {"results": results, "telemetry": telemetry.data}

# === (ê¸°ì¡´) í…ìŠ¤íŠ¸ ì§ì ‘ ë¹„êµ API ===
@app.post("/api/v1/ocr/compare")
def compare(payload: CompareRequest):
    a = payload.left_text or ""
    b = payload.right_text or ""
    set_a = set(a.split())
    set_b = set(b.split())
    only_a = sorted(list(set_a - set_b))[:100]
    only_b = sorted(list(set_b - set_a))[:100]
    overlap = sorted(list(set_a & set_b))[:100]
    return {
        "left_unique_terms_preview": only_a,
        "right_unique_terms_preview": only_b,
        "overlap_terms_preview": overlap,
        "left_len": len(a),
        "right_len": len(b),
    }

# === (ì‹ ê·œ) ì›ë³¸(doc_id) ê¸°ì¤€ ë¹„êµ API ===
@app.post("/api/v1/ocr/compare_by_id")
def compare_by_id(payload: CompareByIdRequest):
    left = _load_doc(payload.left_id)
    right = _load_doc(payload.right_id)

    def norm(s: str) -> str:
        s = unicodedata.normalize("NFC", s)
        s = re.sub(r"[ \t]+", " ", s)
        s = re.sub(r"\n{2,}", "\n", s)
        return s.strip()

    a = norm(left.get("full_text", "") or "")
    b = norm(right.get("full_text", "") or "")

    ta = re.findall(r"\w+", a)
    tb = re.findall(r"\w+", b)
    sa, sb = set(ta), set(tb)

    return {
        "mode": "text",
        "left_len": len(a),
        "right_len": len(b),
        "jaccard_overlap": round(len(sa & sb) / max(1, len(sa | sb)), 4),
        "left_unique_terms_preview": sorted(list(sa - sb))[:100],
        "right_unique_terms_preview": sorted(list(sb - sa))[:100],
    }

# === ZIP ì¼ê´„ ì—…ë¡œë“œ ===
@app.post("/api/v1/ocr/zip")
async def upload_zip(zip_file: UploadFile = File(...)):
    perf = PerfRecorder(enabled=os.getenv("PERF_ENABLED", "false").lower() == "true")
    telemetry = Telemetry()
    data = await zip_file.read()
    memzip = io.BytesIO()
    with zipfile.ZipFile(memzip, 'w', zipfile.ZIP_DEFLATED) as zout:
        for name, text, pages in batch_ocr_zip(data):
            with perf.step(f"llm:{name}"):
                # â¬‡ï¸ ZIP ì²˜ë¦¬ë„ LLMì€ ì›Œì»¤ ìŠ¤ë ˆë“œë¡œ
                summary = await anyio.to_thread.run_sync(summarize_and_categorize, text)
            result = {
                "filename": name,
                "pages": pages,
                "summary": summary
            }
            zout.writestr(f"{name}.json", json.dumps(result, ensure_ascii=False, indent=2))
    telemetry.merge(perf.to_telemetry())
    memzip.seek(0)
    headers = {"X-Telemetry": json.dumps(telemetry.data)}
    return StreamingResponse(memzip, media_type="application/zip", headers=headers)

# === ì²­í¬ ì—…ë¡œë“œ ì„¸ì…˜ ===
@app.post("/api/v1/upload/session")
def create_session():
    sid = str(uuid.uuid4())
    os.makedirs(f"{SESS_ROOT}", exist_ok=True)
    open(f"{SESS_ROOT}/{sid}.part", "wb").close()
    return {"session_id": sid}

@app.patch("/api/v1/upload/session/{sid}")
async def append_chunk(sid: str, chunk: UploadFile = File(...)):
    path = f"{SESS_ROOT}/{sid}.part"
    if not os.path.exists(path):
        return JSONResponse(status_code=404, content={"error": "session not found"})
    data = await chunk.read()
    with open(path, "ab") as f:
        f.write(data)
    return {"ok": True, "bytes": len(data)}

@app.post("/api/v1/upload/session/{sid}/finalize")
async def finalize_session(sid: str, is_zip: bool = Form(True)):
    path = f"{SESS_ROOT}/{sid}.part"
    if not os.path.exists(path):
        return JSONResponse(status_code=404, content={"error": "session not found"})
    with open(path, "rb") as f:
        blob = bytes(f.read())
    os.remove(path)
    if is_zip:
        memzip = io.BytesIO()
        with zipfile.ZipFile(memzip, 'w', zipfile.ZIP_DEFLATED) as zout:
            for name, text, pages in batch_ocr_zip(blob):
                # â¬‡ï¸ ZIP finalizeì—ì„œë„ LLMì€ ì›Œì»¤ ìŠ¤ë ˆë“œë¡œ
                summary = await anyio.to_thread.run_sync(summarize_and_categorize, text)
                result = {"filename": name, "pages": pages, "summary": summary}
                zout.writestr(f"{name}.json", json.dumps(result, ensure_ascii=False, indent=2))
        memzip.seek(0)
        return StreamingResponse(memzip, media_type="application/zip")
    else:
        # â¬‡ï¸ OCR (ë™ê¸°) + ìš”ì•½(ì›Œì»¤ ìŠ¤ë ˆë“œ)
        text, pages, meta, per_page_texts = ocr_funnel_extract(blob, filename="upload.bin", mode="quality")
        summary = await anyio.to_thread.run_sync(summarize_and_categorize, text)

        doc_id = str(uuid.uuid4())
        _store_doc(doc_id, {
            "doc_id": doc_id,
            "filename": "upload.bin",
            "pages": pages,
            "full_text": text,
            "per_page": per_page_texts,
            "meta": meta,
            "summary": summary,
            "ts": int(time.time()),
        })
        return {"doc_id": doc_id, "pages": pages, "summary": summary}
    

from app.services.capcha import router as captcha_router   
from app.services.signup import router as signup_router
from app.services.login import router as login_router
from app.routers.admin_router import router as admin_router
from app.routers.user_check_router import router as user_check_router
from app.routers.email_verify_router import router as email_verify_router

app.include_router(captcha_router)
app.include_router(signup_router)
app.include_router(login_router)
app.include_router(admin_router)
app.include_router(user_check_router)
app.include_router(email_verify_router)