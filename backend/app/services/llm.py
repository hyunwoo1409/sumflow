# services/llm.py
# -------------------------------------------------------------
# ê³ í’ˆì§ˆ ìš”ì•½ íŒŒì´í”„ë¼ì¸
# - ì‚¬ì „ ì •ê·œí™”(í—¤ë”/í˜ì´ì§€ë²ˆí˜¸/ì¡ìŒ ì œê±°)
# - ë§µ-ë¦¬ë“€ìŠ¤(ë¶„í•  ìš”ì•½ â†’ í†µí•© ìš”ì•½)
# - ë°˜(å)ë³µë¶™ ê·œì¹™(8ë‹¨ì–´ ì—°ì† ê¸ˆì§€) + í›„ì²˜ë¦¬ ê²€ì¶œ
# - í•œêµ­ì–´ ì •ì±…/ë²•ì•ˆ ë¬¸ì„œ ë¶„ë¥˜ ê°€ì´ë“œ + ì˜ˆì‹œ ê°•í™”
# - Ollama /api/chat ì‚¬ìš© + ë¹ˆì‘ë‹µ ì‹œ /api/generate ì¬ì‹œë„
# -------------------------------------------------------------

import os
import re
import json
import time
import logging
from typing import Any, Dict, List, Tuple

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger("app")

# ===== í™˜ê²½ ë³€ìˆ˜ =====
OLLAMA_HOST: str = os.getenv("OLLAMA_HOST", "http://localhost:11434").rstrip("/")
OLLAMA_MODEL: str = os.getenv("OLLAMA_MODEL", "llama3")
OLLAMA_TIMEOUT: int = int(os.getenv("OLLAMA_TIMEOUT", "300"))

def _parse_ollama_options() -> Dict[str, Any]:
    """
    OLLAMA_OPTIONS="temperature=0.25,num_predict=512,top_p=0.9"
    ë¯¸ì„¤ì • ì‹œ ë³µë¶™ ì–µì œì™€ ê³¼ë‹¤ì¶œë ¥ ë°©ì§€ì— ìœ ë¦¬í•œ ê¸°ë³¸ê°’ ì‚¬ìš©.
    """
    raw = os.getenv("OLLAMA_OPTIONS", "").strip()
    base = {"temperature": 0.25, "top_p": 0.9, "num_predict": 512}
    if not raw:
        return base
    opts: Dict[str, Any] = {}
    for token in raw.split(","):
        token = token.strip()
        if not token or "=" not in token:
            continue
        k, v = token.split("=", 1)
        k, v = k.strip(), v.strip()
        if v.isdigit():
            opts[k] = int(v)
        elif v.lower() in ("true", "false"):
            opts[k] = (v.lower() == "true")
        else:
            try:
                opts[k] = float(v)
            except ValueError:
                opts[k] = v
    base.update(opts)
    return base

OLLAMA_OPTIONS: Dict[str, Any] = _parse_ollama_options()

# ===== HTTP ì„¸ì…˜ (ì†ë„/ì•ˆì •ì„±) =====
_SESSION = requests.Session()
_ADAPTER = HTTPAdapter(
    pool_connections=16,
    pool_maxsize=16,
    max_retries=Retry(
        total=2,
        backoff_factor=0.2,
        status_forcelist=[502, 503, 504],
        raise_on_status=False,
    ),
)
_SESSION.mount("http://", _ADAPTER)
_SESSION.mount("https://", _ADAPTER)

# ===== ì‚¬ì „ ì •ê·œí™”(í•µì‹¬) =====
_ZWS = "\u200b\u200c\u200d\uFEFF"

HEADER_PATTERNS = [
    r"^\s*-\s*\d+\s*-\s*$",                    # "- 1 -" í˜•íƒœ í˜ì´ì§€ ë§ˆì»¤
    r"^\s*Page\s+\d+\s*(of\s+\d+)?\s*$",      # Page 1 (of 6)
    r"^\s*\d+\s*/\s*\d+\s*$",                 # "1/6"
    r"^\s*ëª©\s*ì°¨\s*$",                        # "ëª©ì°¨" ë‹¨ë…í–‰
]

# (íŒ¨í„´, ì¹˜í™˜ ë¬¸ìì—´)
INLINE_NOISE = [
    (r"\s{2,}", " "),        # ì—¬ëŸ¬ ê³µë°± -> í•œ ì¹¸
    (r"[ \t]+(\n)", r"\1"),  # ì¤„ ë ê³µë°± ì œê±°(ìº¡ì²˜ê·¸ë£¹ ì‚¬ìš©)
]

def _strip_headers_footers(text: str) -> str:
    lines = [ln for ln in (text or "").splitlines()]
    keep: List[str] = []
    for ln in lines:
        s = ln.strip()
        if not s:
            keep.append("")
            continue
        if any(re.match(p, s) for p in HEADER_PATTERNS):
            continue
        keep.append(ln)

    out = "\n".join(keep)
    out = re.sub(rf"[{_ZWS}]", "", out)               # zero-width ì œê±°
    out = re.sub(r"([^\d])-(\s*\n)", r"\1\n", out)    # ì¤„ë í•˜ì´í”ˆ ì¤„ë°”ê¿ˆ ì •ë¦¬
    out = re.sub(r"(?m)^\s*\d+\.\s*", "", out)        # ì¤„ ë²ˆí˜¸ "1. " ì œê±°
    for pat, repl in INLINE_NOISE:
        out = re.sub(pat, repl, out)
    out = re.sub(r"[â”€â”â•\-_=]{5,}", "", out)           # ê¸´ êµ¬ë¶„ì„  ì œê±°
    return out.strip()

def _preclean(text: str) -> str:
    t = (text or "").replace("\r", "")
    before = len(t)
    t = _strip_headers_footers(t)
    t = re.sub(r"\n{3,}", "\n\n", t)  # ë¹ˆì¤„ ê³¼ì¶•ì†Œ ë°©ì§€
    after = len(t)
    logger.info(f"[LLM] preclean: len_before={before}, len_after={after}")
    return t

# ===== í”„ë¡¬í”„íŠ¸ =====
SYS_SUMMARY = (
    "You are an expert summarizer for Korean government and legislative documents. "
    "Return Korean if the source is Korean. Produce short, abstract summaries that do not copy long spans of the source. "
    "Do NOT copy any sentence verbatim; limit any direct quote to <= 8 consecutive words. "
    "Be precise about policy changes, legal clauses, thresholds, dates."
)

CATEGORY_GUIDE = """
ë¶„ë¥˜ ê°€ì´ë“œ(ì˜ˆì‹œ):
- ì •ì±…/ë²•ì•ˆ / êµ­íšŒë³´ê³ 
- ì •ì±…/ë²•ì•ˆ / ì²´ê³„ìêµ¬ê²€í† 
- ì‚°ì—…/ìˆ˜ì‚° / ì¡°í•©Â·í˜‘ë™ì¡°í•©
- ì¡°ì§/ì¸ì‚¬ / ì—¬ì„±ì°¸ì—¬Â·í• ë‹¹
- ì¬ì •/ì§€ì› / ë³´ì¡°Â·ì§€ì›ì œë„
ë¬¸ì„œì— ë§ì¶° [ëŒ€ë¶„ë¥˜]/[ì†Œë¶„ë¥˜]ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ì„ íƒí•˜ì„¸ìš”.
"""

CHUNK_USER_PROMPT = (
    "ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ 3~6ê°œì˜ í•µì‹¬ ë¶ˆë¦¿ìœ¼ë¡œ **ê°„ê²°í•œ ì¶”ìƒí™” ìš”ì•½**í•´ì¤˜.\n"
    "- ì›ë¬¸ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì‚¬ ê¸ˆì§€(8ë‹¨ì–´ ì—°ì† ê¸ˆì§€)\n"
    "- ì •ì±…/ì¡°ë¬¸/ìˆ˜ì¹˜/ëŒ€ìƒ/íš¨ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½\n"
    "- ë¶ˆë¦¿ 1ê°œëŠ” 120ì ì´ë‚´\n"
    "- JSONë§Œ ë°˜í™˜\n\n"
    "{{\n"
    '  "bullets": ["...", "...", "..."]\n'
    "}}\n\n"
    "í…ìŠ¤íŠ¸:\n"
    "{body}\n"
)


REDUCE_USER_PROMPT = (
    "ì—¬ëŸ¬ ì¡°ê° ìš”ì•½ì„ í†µí•©í•´ **ë¬¸ì„œ ì „ì²´ ìš”ì•½**ì„ ì‘ì„±í•˜ì„¸ìš”.\n"
    "- ì œëª© 1ì¤„(80ì ì´ë‚´)\n"
    "- bullets 4~8ê°œ, ê° 120ì ì´ë‚´, ì¤‘ë³µÂ·êµ°ë”ë”ê¸° ì œê±°, ì¶”ìƒí™” ìš”ì•½\n"
    "- category/subcategoryëŠ” í•œêµ­ì–´ë¡œ, ì•„ë˜ ê°€ì´ë“œë¥¼ ì°¸ê³ í•´ ì„ íƒ\n"
    f"{CATEGORY_GUIDE}\n"
    "- ì›ë¬¸ ë¬¸ì¥ ë³µì‚¬ ê¸ˆì§€(8ë‹¨ì–´ ì—°ì† ê¸ˆì§€)\n"
    "- JSONë§Œ ë°˜í™˜\n\n"
    "{\n"
    '  "title": "...",\n'
    '  "bullets": ["...", "..."],\n'
    '  "category": "ì •ì±…/ë²•ì•ˆ",\n'
    '  "subcategory": "êµ­íšŒë³´ê³ "\n'
    "}\n\n"
    "ì¡°ê° ìš”ì•½ë“¤(JSON ë°°ì—´):\n"
    "{chunks}\n"
)

# ===== ê³µí†µ ìœ í‹¸: Ollama í˜¸ì¶œ =====
def _ollama_chat(messages: List[Dict[str, str]]) -> str:
    url = f"{OLLAMA_HOST}/api/chat"
    payload: Dict[str, Any] = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": False,
        "options": OLLAMA_OPTIONS,
    }
    t0 = time.perf_counter()
    try:
        logger.info(f"ğŸ§  LLM call START (chat) model={OLLAMA_MODEL}")
        resp = _SESSION.post(url, json=payload, timeout=OLLAMA_TIMEOUT)
        resp.raise_for_status()
        data = resp.json()
        content = (data.get("message") or {}).get("content", "") or ""
        dt = int((time.perf_counter() - t0) * 1000)
        logger.info(f"ğŸ§  LLM call DONE (chat) in {dt} ms; len={len(content)}")
        return content
    except Exception as e:
        dt = int((time.perf_counter() - t0) * 1000)
        logger.exception(f"ğŸ§  LLM call ERROR (chat) after {dt} ms: {e}")
        return ""

def _ollama_generate(prompt: str, system: str = "") -> str:
    url = f"{OLLAMA_HOST}/api/generate"
    full_prompt = (system + "\n\n" + prompt) if system else prompt
    payload: Dict[str, Any] = {
        "model": OLLAMA_MODEL,
        "prompt": full_prompt,
        "stream": False,
        "options": OLLAMA_OPTIONS,
    }
    t0 = time.perf_counter()
    try:
        logger.info(f"ğŸ§  LLM call START (generate) model={OLLAMA_MODEL}")
        resp = _SESSION.post(url, json=payload, timeout=OLLAMA_TIMEOUT)
        resp.raise_for_status()
        data = resp.json()
        content = data.get("response", "") or ""
        dt = int((time.perf_counter() - t0) * 1000)
        logger.info(f"ğŸ§  LLM call DONE (generate) in {dt} ms; len={len(content)}")
        return content
    except Exception as e:
        dt = int((time.perf_counter() - t0) * 1000)
        logger.exception(f"ğŸ§  LLM call ERROR (generate) after {dt} ms: {e}")
        return ""

def _safe_json_parse(s: str) -> Dict[str, Any]:
    try:
        return json.loads(s)
    except Exception:
        pass
    m = re.search(r"\{[\s\S]*\}", s)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            return {}
    return {}

def _clip_list_str(xs: List[str], n_max: int, each_len: int) -> List[str]:
    out: List[str] = []
    for x in xs[:n_max]:
        y = (x or "").strip()
        if not y:
            continue
        if len(y) > each_len:
            y = y[:each_len].rstrip() + "â€¦"
        out.append(y)
    return out

def _contains_long_verbatim(bullet: str, source: str, limit_words: int = 8) -> bool:
    bw = re.findall(r"\w+", bullet)
    if len(bw) < limit_words:
        return False
    for i in range(0, len(bw) - limit_words + 1):
        phrase = " ".join(bw[i:i+limit_words])
        if phrase and phrase in source:
            return True
    return False

def _remove_overly_verbatim(bullets: List[str], source: str) -> List[str]:
    cleaned: List[str] = []
    for b in bullets:
        if not _contains_long_verbatim(b, source, limit_words=8):
            cleaned.append(b)
    return cleaned or bullets

def _split_text(text: str, max_chars: int = 1800) -> List[str]:
    s = (text or "").strip()
    if not s:
        return []
    if len(s) <= max_chars:
        return [s]
    parts, buf, acc = [], [], 0
    paras = re.split(r"\n{2,}", s)
    for p in paras:
        p = p.strip()
        if not p:
            continue
        if len(p) > max_chars:
            sents = re.split(r"(?<=[\.!?])\s+|(?<=[ë‹¤ìš”]\.)\s+", p)
            for sent in sents:
                sent = sent.strip()
                if not sent:
                    continue
                if acc + len(sent) + 1 > max_chars and buf:
                    parts.append("\n".join(buf)); buf, acc = [], 0
                buf.append(sent); acc += len(sent) + 1
        else:
            if acc + len(p) + 2 > max_chars and buf:
                parts.append("\n".join(buf)); buf, acc = [], 0
            buf.append(p); acc += len(p) + 2
    if buf:
        parts.append("\n".join(buf))
    return parts or [s[:max_chars]]

# ===== ê·œì¹™ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë³´ì • =====
def _rule_category(text: str) -> Tuple[str, str]:
    t = (text or "").replace(" ", "")
    if "ì²´ê³„ìêµ¬ê²€í† " in t:
        return ("ì •ì±…/ë²•ì•ˆ", "ì²´ê³„ìêµ¬ê²€í† ")
    if "ìœ„ì›íšŒì˜ê²°ì•ˆ" in t:
        return ("ì •ì±…/ë²•ì•ˆ", "êµ­íšŒë³´ê³ ")
    if "ê²€í† ë³´ê³ ì„œ" in t:
        return ("ì •ì±…/ë²•ì•ˆ", "ê²€í† ë³´ê³ ")
    if "ìˆ˜ì‚°ì—…í˜‘ë™ì¡°í•©ë²•" in t:
        return ("ì •ì±…/ë²•ì•ˆ", "ì¡°í•©Â·í˜‘ë™ì¡°í•©")
    if "ì—¬ì„±ì„ì›" in t or "ì—¬ì„±ì°¸ì—¬" in t:
        return ("ì¡°ì§/ì¸ì‚¬", "ì—¬ì„±ì°¸ì—¬Â·í• ë‹¹")
    return ("ë¯¸ë¶„ë¥˜", "ë¯¸ë¶„ë¥˜")

# ===== ìš”ì•½ íŒŒì´í”„ë¼ì¸ =====
def _summarize_chunk(chunk: str) -> List[str]:
    messages = [
        {"role": "system", "content": SYS_SUMMARY},
        {"role": "user", "content": CHUNK_USER_PROMPT.format(body=chunk)},
    ]
    content = _ollama_chat(messages)
    if not content or len(content) < 5:
        logger.warning("[LLM] empty/short chat result; retrying with /api/generate")
        content = _ollama_generate(CHUNK_USER_PROMPT.format(body=chunk), system=SYS_SUMMARY)

    obj = _safe_json_parse(content)
    bullets = obj.get("bullets", [])
    if not isinstance(bullets, list):
        bullets = [str(bullets)]
    bullets = [str(b).strip() for b in bullets if str(b).strip()]
    bullets = _clip_list_str(bullets, n_max=8, each_len=140)
    bullets = _remove_overly_verbatim(bullets, chunk)
    return bullets

def _reduce_summaries(all_bullets: List[List[str]]) -> Dict[str, Any]:
    chunk_objs = [{"bullets": blts} for blts in all_bullets if blts]
    reduce_prompt = REDUCE_USER_PROMPT.format(chunks=json.dumps(chunk_objs, ensure_ascii=False))

    messages = [
        {"role": "system", "content": SYS_SUMMARY},
        {"role": "user", "content": reduce_prompt},
    ]
    content = _ollama_chat(messages)
    if not content or len(content) < 5:
        logger.warning("[LLM] empty/short chat result in reduce; retrying with /api/generate")
        content = _ollama_generate(reduce_prompt, system=SYS_SUMMARY)

    obj = _safe_json_parse(content)

    title = str(obj.get("title", "")).strip()[:120] or "Untitled"
    bullets = obj.get("bullets", [])
    if not isinstance(bullets, list):
        bullets = [str(bullets)]
    bullets = [str(b).strip() for b in bullets if str(b).strip()]
    bullets = _clip_list_str(bullets, n_max=8, each_len=140)

    cat = str(obj.get("category", "")).strip()
    sub = str(obj.get("subcategory", "")).strip()
    return {
        "title": title,
        "bullets": bullets,
        "category": cat or "ë¯¸ë¶„ë¥˜",
        "subcategory": sub or "ë¯¸ë¶„ë¥˜",
    }

def _fallback_summary(text: str) -> Dict[str, Any]:
    head = (text or "").strip().splitlines()
    title = (head[0] if head else "Untitled").strip()[:120]
    body = " ".join(head[1:])[:600]
    bullets = [body[i:i+100] for i in range(0, len(body), 100)][:5] or [title]
    return {
        "title": title or "Untitled",
        "bullets": bullets,
        "category": "ë¯¸ë¶„ë¥˜",
        "subcategory": "ë¯¸ë¶„ë¥˜",
    }

def summarize_and_categorize(text: str) -> Dict[str, Any]:
    try:
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ë¡œì»¬ ê·œì¹™ ë¶„ë¥˜ê¸°(ì—†ìœ¼ë©´ ì—¬ê¸° ê±¸ë¡œ ë³´ì •)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def _rule_category_local(src: str) -> Tuple[str, str]:
            t = (src or "").replace(" ", "")
            if "ì²´ê³„ìêµ¬ê²€í† " in t:
                return ("ì •ì±…/ë²•ì•ˆ", "ì²´ê³„ìêµ¬ê²€í† ")
            if "ìœ„ì›íšŒì˜ê²°ì•ˆ" in t or "ì˜ê²°ì•ˆ" in t:
                return ("ì •ì±…/ë²•ì•ˆ", "êµ­íšŒë³´ê³ ")
            if "ê²€í† ë³´ê³ ì„œ" in t or "ê²€í† ë³´ê³ " in t:
                return ("ì •ì±…/ë²•ì•ˆ", "ê²€í† ë³´ê³ ")
            if "ìˆ˜ì‚°ì—…í˜‘ë™ì¡°í•©ë²•" in t or "í˜‘ë™ì¡°í•©" in t:
                return ("ì •ì±…/ë²•ì•ˆ", "ì¡°í•©Â·í˜‘ë™ì¡°í•©")
            if "ì—¬ì„±ì„ì›" in t or "ì—¬ì„±ì°¸ì—¬" in t or "ì—¬ì„±í• ë‹¹" in t:
                return ("ì¡°ì§/ì¸ì‚¬", "ì—¬ì„±ì°¸ì—¬Â·í• ë‹¹")
            return ("ë¯¸ë¶„ë¥˜", "ë¯¸ë¶„ë¥˜")

        cleaned = _preclean(text)
        if not cleaned and text:
            cleaned = text  # ê³¼ì‰ ì •ê·œí™” ë°©ì§€

        # === ë¹ ë¥¸ ê²½ë¡œ: ì§§ì€ ë¬¸ì„œëŠ” ì›íŒ¨ìŠ¤ ìš”ì•½ìœ¼ë¡œ ëë‚´ê¸° ===
        QUICK_N = int(os.getenv("SUMM_QUICK_THRESHOLD", "2500"))
        if len(cleaned) <= QUICK_N:
            logger.info(f"[LLM] quick path: single-pass summarize (len={len(cleaned)})")
            # chat ì‹œë„
            content = _ollama_chat([
                {"role": "system", "content": SYS_SUMMARY},
                {"role": "user", "content": REDUCE_USER_PROMPT.format(
                    chunks=json.dumps([{"bullets": []}], ensure_ascii=False)
                ) + "\n\nì›ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸:\n" + cleaned[:8000]}
            ])
            # ë¹ˆ/ì§§ìœ¼ë©´ generate ì¬ì‹œë„
            if not content or len(content) < 5:
                logger.warning("[LLM] quick path empty/short; retrying with /api/generate")
                prompt = REDUCE_USER_PROMPT.format(
                    chunks=json.dumps([{"bullets": []}], ensure_ascii=False)
                ) + "\n\nì›ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸:\n" + cleaned[:8000]
                content = _ollama_generate(prompt, system=SYS_SUMMARY)

            obj = _safe_json_parse(content)
            title = (str(obj.get("title", "")) or "Untitled")[:120]
            bullets = obj.get("bullets", [])
            if not isinstance(bullets, list):
                bullets = [str(bullets)]
            bullets = _clip_list_str([str(b).strip() for b in bullets if str(b).strip()], n_max=8, each_len=140)
            cat = (str(obj.get("category", "")) or "").strip()
            sub = (str(obj.get("subcategory", "")) or "").strip()
            if not cat or not sub or cat == "ë¯¸ë¶„ë¥˜" or sub == "ë¯¸ë¶„ë¥˜":
                rc, rs = _rule_category_local(cleaned)
                cat, sub = rc, rs
            return {
                "title": title,
                "bullets": bullets,
                "category": cat or "ë¯¸ë¶„ë¥˜",
                "subcategory": sub or "ë¯¸ë¶„ë¥˜",
            }

        # === ê¸°ì¡´(ë§µ-ë¦¬ë“€ìŠ¤) ê²½ë¡œ ===
        logger.info(f"[LLM] input lengths: raw={len(text or '')}, cleaned={len(cleaned)}")
        chunks = _split_text(cleaned, max_chars=1800)
        logger.info(f"[LLM] chunk_count={len(chunks)}, chunk_lens={[len(c) for c in chunks[:3]]}{'...' if len(chunks)>3 else ''}")

        if not chunks and cleaned:
            chunks = [cleaned[:1800]]
            logger.warning("[LLM] chunks empty after split; forcing single chunk to call Ollama.")

        if not chunks:
            logger.warning("[LLM] no content after cleaning; using fallback.")
            return _fallback_summary(cleaned)

        all_bullets: List[List[str]] = []
        for idx, ch in enumerate(chunks):
            logger.info(f"[LLM] summarize chunk {idx+1}/{len(chunks)} len={len(ch)}")
            blts = _summarize_chunk(ch)
            logger.info(f"[LLM] chunk {idx+1} bullets={len(blts)}")
            if blts:
                all_bullets.append(blts)

        if not all_bullets:
            logger.warning("[LLM] all chunks empty; using fallback.")
            return _fallback_summary(cleaned)

        # === ë¦¬ë“€ìŠ¤ ìŠ¤í‚µ: ë§µ ê²°ê³¼ê°€ ì ìœ¼ë©´ ë°”ë¡œ í•©ì¹˜ê¸° ===
        if len(all_bullets) <= 2:
            flat = [b for blts in all_bullets for b in blts]
            flat = _clip_list_str(flat, n_max=8, each_len=140)
            rc, rs = _rule_category_local(cleaned)
            title_guess = (cleaned.splitlines()[0] if cleaned else "Untitled").strip()[:120]
            return {
                "title": title_guess or "Untitled",
                "bullets": flat,
                "category": rc,
                "subcategory": rs,
            }

        logger.info(f"[LLM] reduce {len(all_bullets)} chunk-summaries")
        result = _reduce_summaries(all_bullets)

        # ìµœì¢… ê¸¸ì´/ì•ˆì „ ë³´ì •
        result["bullets"] = _clip_list_str(result.get("bullets", []), n_max=8, each_len=140)
        result["title"] = (result.get("title") or "Untitled")[:120]

        # â¬‡ï¸ ì¹´í…Œê³ ë¦¬ ë¹„ì—ˆê±°ë‚˜ ë¯¸ë¶„ë¥˜ë©´ ê·œì¹™ ë³´ì •
        cat, sub = result.get("category") or "", result.get("subcategory") or ""
        if not cat or not sub or cat == "ë¯¸ë¶„ë¥˜" or sub == "ë¯¸ë¶„ë¥˜":
            rc, rs = _rule_category_local(cleaned)
            result["category"] = rc
            result["subcategory"] = rs

        return result

    except Exception as e:
        logger.warning(f"LLM summarize fallback used due to: {e}")
        return _fallback_summary(text)
